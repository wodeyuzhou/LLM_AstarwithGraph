{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9805fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import heapq\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from graph_to_prompt_qwen import graph_to_prompt, graph_to_prompt_opt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92a62b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nums_list(s):\n",
    "    nums_str = re.findall(r'\\d+', s)  \n",
    "    nums = list(map(int, nums_str))\n",
    "    return  nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7249a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Request_llm:\n",
    "    def __init__(self, model_name):\n",
    "        if model_name == 'llama':\n",
    "            self.max_new_tokens = 200\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", device_map=\"auto\",load_in_8bit=True)\n",
    "            #self.model.save_model(\"models/meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "        if model_name == 'deepseek':\n",
    "            self.max_new_tokens = 50\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", device_map=\"auto\",load_in_8bit=True)\n",
    "\n",
    "        if model_name == 'qwen':\n",
    "            self.max_new_tokens = 200\n",
    "            rope = {\n",
    "                \"rope_type\": \"yarn\",\n",
    "                \"factor\": 4.0,                         \n",
    "                \"original_max_position_embeddings\": 32768\n",
    "            }\n",
    "\n",
    "            cfg = AutoConfig.from_pretrained(\n",
    "                    \"Qwen/Qwen3-8B\",\n",
    "                    rope_scaling=rope\n",
    "                    )\n",
    "            \n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-8B\", config=cfg, device_map=\"auto\", load_in_8bit=True) \n",
    "\n",
    "        self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "\n",
    "    def get_waypoints(self, G, start, goal, n_points):\n",
    "\n",
    "        messages = graph_to_prompt_opt(G, start, goal, n_points)\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    "        )\n",
    "        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        input_ids = model_inputs['input_ids']\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,          # 리스트만 나오면 충분\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "        # parsing thinking content\n",
    "        try:\n",
    "            # rindex finding 151668 (</think>)\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "        print(content)\n",
    "\n",
    "        \"\"\"\n",
    "        prompt = graph_to_prompt(G, start, goal, n_points)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    return_attention_mask=True,\n",
    "                    truncation=True     # 입력도 길면 자르기\n",
    "                ).to(self.model.device)\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].to(self.model.device)\n",
    "        inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(self.model.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,  # ← 필수\n",
    "                num_beams=1,\n",
    "                use_cache=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        generated_token = outputs[:, input_ids.shape[-1] :]\n",
    "        result = self.tokenizer.batch_decode(generated_token, skip_special_tokens=True)[0]\n",
    "        waypoints = get_nums_list(result)\"\"\"\n",
    "\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37888e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar(G, start, goal, heuristic, obstacles = None):\n",
    "    count = 0\n",
    "    storage = 0\n",
    "    checking_edges = set()\n",
    "\n",
    "    if obstacles is None:\n",
    "        obstacles = set()\n",
    "    else:\n",
    "        obstacles = set(obstacles)\n",
    "\n",
    "    # g-score 와 f-score 초기화\n",
    "    g_score = {n: float('inf') for n in G.nodes}\n",
    "    g_score[start] = 0\n",
    "\n",
    "    f_score = {n: float('inf') for n in G.nodes}\n",
    "    f_score[start] = heuristic[start]\n",
    "\n",
    "    # 우선순위 큐: (f_score, g_score, node)\n",
    "    open_set = []\n",
    "    heapq.heappush(open_set, (f_score[start], 0, start))\n",
    "    visited = set()\n",
    "    came_from = {}\n",
    "\n",
    "    num_iter = 0\n",
    "\n",
    "    while open_set:\n",
    "        num_iter += 1\n",
    "        storage += len(open_set) + len(visited) + len(came_from)\n",
    "        f_cur, g_cur, current = heapq.heappop(open_set)\n",
    "\n",
    "        # 목표 도달\n",
    "        if current == goal:\n",
    "            # 경로 재구성\n",
    "            path = [current]\n",
    "            while current in came_from:\n",
    "                current = came_from[current]\n",
    "                path.append(current)\n",
    "            return path[::-1], g_cur, count, storage, list(checking_edges)\n",
    "        visited.add(current)\n",
    "        # 이웃 탐색\n",
    "        for nbr in G.neighbors(current):\n",
    "            count = count + 1\n",
    "            if nbr in visited or nbr in obstacles:\n",
    "                continue\n",
    "            \n",
    "            tentative_g = g_score[current] + G[current][nbr]['weight']\n",
    "            checking_edges.add((current, nbr))\n",
    "            if tentative_g < g_score[nbr]:\n",
    "                came_from[nbr] = current\n",
    "                g_score[nbr] = tentative_g\n",
    "                f_score[nbr] = tentative_g + heuristic[nbr]\n",
    "                heapq.heappush(open_set, (f_score[nbr], tentative_g, nbr))\n",
    "\n",
    "    # 경로가 없을 때\n",
    "    return None, None, count, storage, list(checking_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc18a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_astar(G, start, goal, heuristic, llm, obstacles=None):\n",
    "    nodes = G.nodes\n",
    "    llm = list(set(llm))\n",
    "    T = []\n",
    "    for ts in llm:\n",
    "        if ts in nodes:\n",
    "            T.append(ts)\n",
    "\n",
    "    if obstacles is None:\n",
    "        obstacles = set()\n",
    "    else:\n",
    "        obstacles = set(obstacles)\n",
    "        \n",
    "    print(T)\n",
    "    # T.reverse()\n",
    "    if not T or T[0] != start:\n",
    "        T.insert(0, start)\n",
    "    if T[-1] != goal:\n",
    "        T.append(goal)\n",
    "\n",
    "    t_idx = 1 if len(T) > 1 else 0\n",
    "    t = T[t_idx]\n",
    "    dist_to_t = nx.single_source_dijkstra_path_length(G, t, weight='weight')\n",
    "    count = 0\n",
    "    storage = 0\n",
    "    checking_edges = set()\n",
    "    came_from = {}\n",
    "    visited = set()\n",
    "\n",
    "    g_score = {n: float('inf') for n in G.nodes}\n",
    "    g_score[start] = 0\n",
    "\n",
    "    f_score = {n: float('inf') for n in G.nodes}\n",
    "    f_score[start] = g_score[start] + heuristic[start] + dist_to_t.get(start, float('inf'))\n",
    "\n",
    "    open_set = []\n",
    "    heapq.heappush(open_set, (f_score[start], g_score[start], start))\n",
    "\n",
    "    while open_set:\n",
    "        storage += len(open_set) + len(visited) + len(came_from)\n",
    "        f_cur, g_cur, current = heapq.heappop(open_set)\n",
    "        if current == goal:\n",
    "            path = [current]\n",
    "            while current in came_from:\n",
    "                current = came_from[current]\n",
    "                path.append(current)\n",
    "            return path[::-1], g_cur, count, storage, list(checking_edges), T\n",
    "        visited.add(current)\n",
    "        if current == t and t_idx < len(T) - 1:\n",
    "            t_idx += 1\n",
    "            t = T[t_idx]\n",
    "            dist_to_t = nx.single_source_dijkstra_path_length(G, t, weight='weight')\n",
    "\n",
    "            new_open = []\n",
    "            for _, g_old, node in open_set:\n",
    "                new_f = g_score[node] + heuristic[node] + dist_to_t.get(node, float('inf'))\n",
    "                new_open.append((new_f, g_old, node))\n",
    "            open_set = new_open\n",
    "            heapq.heapify(open_set)\n",
    "\n",
    "\n",
    "        for nbr in G.successors(current):\n",
    "            if nbr in visited or nbr in obstacles:\n",
    "                continue\n",
    "            count += 1\n",
    "            w = G[current][nbr].get('weight', 1)\n",
    "            tentative_g = g_score[current] + w\n",
    "            checking_edges.add((current, nbr))\n",
    "\n",
    "            if tentative_g < g_score[nbr]:\n",
    "                came_from[nbr] = current\n",
    "                g_score[nbr] = tentative_g\n",
    "                f = tentative_g + heuristic[nbr] + dist_to_t.get(nbr, float('inf'))\n",
    "                f_score[nbr] = f\n",
    "                heapq.heappush(open_set, (f, tentative_g, nbr))\n",
    "\n",
    "\n",
    "    return None, None, count, storage, list(checking_edges), T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cfb0fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/LLM/lib/python3.13/site-packages/networkx/readwrite/json_graph/node_link.py:287: FutureWarning: \n",
      "The default value will be changed to `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_graph(data, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_graph(data, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"../graphs/sejong_bus.json\", \"r\") as json_file:\n",
    "    j = json.load(json_file)\n",
    "\n",
    "G = json_graph.node_link_graph(j)\n",
    "nodes = G.nodes\n",
    "edges = G.edges\n",
    "stations = list(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab31a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table(random) A*\n",
    "experiments_samples_A_star = []\n",
    "\n",
    "while True:\n",
    "    start, goal = np.random.choice(nodes, 2)\n",
    "    start, goal = start.item(), goal.item()\n",
    "\n",
    "    heuristic_table = {station: (((G.nodes[goal]['x'] - G.nodes[station]['x']) ** 2 + (G.nodes[goal]['y'] - G.nodes[station]['y']) ** 2) ** 0.5) for station in nodes}\n",
    "\n",
    "    path, cost, count, storage, checking_edges = astar(G, start, goal, heuristic_table)\n",
    "    if path is not None:\n",
    "        experiments_samples_A_star.append({'point' : (start, goal), 'path':path, 'cost':cost, 'storage':storage, 'count':count, 'checking_edges' : len(checking_edges)})\n",
    "    if len(experiments_samples_A_star)>150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d583c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39707ff99b6145a9a9bb40a2be23759d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_model = Request_llm('qwen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27ad58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca192172b4654e90809558487c1fa228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/LLM/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/LLM/lib/python3.13/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (40960). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6801261, 6801262, 6801263, 6801264, 6801265, 6801266, 6801267, 6800760]\n",
      "[6801261, 6801262, 6801263, 6801264, 6801265, 6801266, 6801267, 6800760]\n",
      "Sample  0\n",
      "Count ratio       : 0.4679334916864608\n",
      "Storage ratio     -0.09181592198355526\n",
      "Cost ratio        : 0.0\n",
      "# of Checking edges ratio: -0.03338632750397452\n",
      "[6800405, 6800001, 6801141, 6801304, 6801485, 6800075, 6800076, 6801174, 6801175, 6801176, 6801177, 6801178, 6801179, 6801180, 6801181, 6801182, 6801183, 6801184, 6801185, 6801186, 6801187, 6801188, 6\n",
      "[6800001, 6800405, 6801174, 6801175, 6801176, 6801304, 6801177, 6801178, 6801179, 6801180, 6801181, 6801188, 6800075, 6800076, 6801485, 6801141]\n",
      "Sample  1\n",
      "Count ratio       : 0.4970414201183432\n",
      "Storage ratio     -0.019295136230022436\n",
      "Cost ratio        : 0.0\n",
      "# of Checking edges ratio: 0.0\n",
      "[6801849, 6801848, 6800006, 6800007, 6801144, 6800009, 6800010, 6800011, 6800012]\n",
      "[6800006, 6800007, 6800009, 6800010, 6800011, 6800012, 6801144, 6801848, 6801849]\n",
      "Sample  2\n",
      "Count ratio       : 0.5130434782608695\n",
      "Storage ratio     0.0009918894097431519\n",
      "Cost ratio        : 0.0\n",
      "# of Checking edges ratio: 0.0\n",
      "[6800687, 6800001, 6800002, 6800004, 6800005, 6800006, 6800007, 6800008, 6800009, 6800010, 6800011, 6800013, 6800014, 6800015, 6800016, 6800017, 6800018, 6800019, 6800020, 6800021, 6800022, 6800023, 6\n",
      "[6800001, 6800002, 6800004, 6800005, 6800006, 6800007, 6800008, 6800009, 6800010, 6800011, 6800013, 6800014, 6800015, 6800016, 6800017, 6800018, 6800019, 6800020, 6800021, 6800022, 6800023, 6800687]\n",
      "Sample  3\n",
      "Count ratio       : 0.7659574468085106\n",
      "Storage ratio     0.8142366240106977\n",
      "Cost ratio        : 0.0\n",
      "# of Checking edges ratio: 0.555735056542811\n",
      "[6800067, 6800008, 6800040, 6800041, 6800042, 6800043, 6800044, 6800045, 6805492]\n",
      "[6800067, 6800008, 6800041, 6800042, 6800040, 6800043, 6800044, 6800045, 6805492]\n",
      "Sample  4\n",
      "Count ratio       : 0.0\n",
      "Storage ratio     0.0\n",
      "Cost ratio        : 0.0\n",
      "# of Checking edges ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# table(random) LLM-A*\n",
    "num_request = 7\n",
    "llm_model.model.eval()\n",
    "ctx = torch.inference_mode  # 또는 torch.no_grad\n",
    "experiments_samples_LLM_A_star = []\n",
    "\n",
    "with ctx():\n",
    "    for i, sample in (enumerate(tqdm(experiments_samples_A_star[:10]))):\n",
    "        start, goal = sample['point']\n",
    "\n",
    "        heuristic_table = {station: (((G.nodes[goal]['x'] - G.nodes[station]['x']) ** 2 + (G.nodes[goal]['y'] - G.nodes[station]['y']) ** 2) ** 0.5) for station in nodes}\n",
    "\n",
    "        waypoints = get_nums_list(llm_model.get_waypoints(G, start, goal, num_request))\n",
    "        \n",
    "        \n",
    "        path, cost, count, storage, checking_edges, waypoints_using_llm = llm_astar(G, start, goal, heuristic_table, waypoints)\n",
    "        if path is not None:\n",
    "            experiments_samples_LLM_A_star.append(\n",
    "                {'point' : (start, goal), \n",
    "                'path':path, \n",
    "                'cost':cost, \n",
    "                'count':count, \n",
    "                'storage':storage,\n",
    "                'checking_edges' : len(checking_edges),\n",
    "                'waypoints_of_llm' : waypoints,\n",
    "                'waypoints_using_llm' : waypoints_using_llm,\n",
    "                'success' : (len(waypoints) <= num_request + 1)\n",
    "                })\n",
    "            cost_ratio = 1 - cost/sample['cost']\n",
    "            count_ratio = 1 - count/sample['count']\n",
    "            storage_ratio = 1 - storage/sample['storage']\n",
    "            checking_edges_ratio = 1 - len(checking_edges)/sample['checking_edges']\n",
    "            print(\"Sample \", i)\n",
    "            print(\"Count ratio       :\", count_ratio)\n",
    "            print(\"Storage ratio    \",  storage_ratio)\n",
    "            print(\"Cost ratio        :\", cost_ratio)\n",
    "            print(\"# of Checking edges ratio:\", checking_edges_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53013095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'point': (6802345, 6800339),\n",
       " 'path': [6802345,\n",
       "  6800694,\n",
       "  6805618,\n",
       "  6800314,\n",
       "  6800313,\n",
       "  6800312,\n",
       "  6800320,\n",
       "  6800319,\n",
       "  6800862,\n",
       "  6800113,\n",
       "  6800112,\n",
       "  6800974,\n",
       "  6806239,\n",
       "  6800315,\n",
       "  6800109,\n",
       "  6800108,\n",
       "  6801238,\n",
       "  6800107,\n",
       "  6801138,\n",
       "  6800078,\n",
       "  6800077,\n",
       "  6800076,\n",
       "  6800075,\n",
       "  6801485,\n",
       "  6800001,\n",
       "  6800157,\n",
       "  6800158,\n",
       "  6800159,\n",
       "  6800160,\n",
       "  6800338,\n",
       "  6800339],\n",
       " 'cost': 53,\n",
       " 'storage': 384596,\n",
       " 'count': 1490,\n",
       " 'checking_edges': 748}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments_samples_A_star[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = experiments_samples_LLM_A_star\n",
    "\n",
    "fields = ['point', 'path', 'cost', 'count', 'checking_edges', 'waypoints_of_llm', 'waypoints_using_llm', 'success']\n",
    "\n",
    "with open('Experiments_result/LLM_A_star_fewshot.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Experiments_result/A_star.csv')\n",
    "data.loc[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b971c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ast.literal_eval(data[0]['point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_samples_A_star[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Experiments_result/A_star.json', 'w') as jsonfile:\n",
    "\tjson.dump(experiments_samples_A_star, jsonfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074694e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Experiments_result/A_star.json', 'r') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19bab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
