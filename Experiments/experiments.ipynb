{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9805fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import heapq\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from graph_to_prompt import graph_to_prompt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92a62b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nums_list(s):\n",
    "    nums_str = re.findall(r'\\d+', s)  \n",
    "    nums = list(map(int, nums_str))\n",
    "    return  nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7249a916",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1605473239.py, line 22)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif.\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Request_llm:\n",
    "    def __init__(self, model_name):\n",
    "        if model_name == 'llama':\n",
    "            self.max_new_tokens = 200\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", device_map=\"auto\",load_in_8bit=True)\n",
    "            #self.model.save_model(\"models/meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "        if model_name == 'deepseek':\n",
    "            self.max_new_tokens = 50\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\", device_map=\"auto\",load_in_8bit=True)\n",
    "\n",
    "        if model_name == 'qwen':\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B\", device_map=\"auto\",load_in_8bit=True) \n",
    "\n",
    "        self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "\n",
    "    def get_waypoints(self, j, start, goal, n_points):\n",
    "        prompt = graph_to_prompt(j, start, goal, n_points)\n",
    "        if.\n",
    "        inputs = self.tokenizer(\n",
    "                    prompt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    return_attention_mask=True,\n",
    "                    truncation=True     # 입력도 길면 자르기\n",
    "                ).to(self.model.device)\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"].to(self.model.device)\n",
    "        inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(self.model.device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,  # ← 필수\n",
    "                num_beams=1,\n",
    "                use_cache=True,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        generated_token = outputs[:, input_ids.shape[-1] :]\n",
    "        result = self.tokenizer.batch_decode(generated_token, skip_special_tokens=True)[0]\n",
    "        waypoints = get_nums_list(result)\n",
    "        return waypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37888e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar(G, start, goal, heuristic, obstacles = None):\n",
    "    count = 0\n",
    "    storage = 0\n",
    "    checking_edges = set()\n",
    "\n",
    "    if obstacles is None:\n",
    "        obstacles = set()\n",
    "    else:\n",
    "        obstacles = set(obstacles)\n",
    "\n",
    "    # g-score 와 f-score 초기화\n",
    "    g_score = {n: float('inf') for n in G.nodes}\n",
    "    g_score[start] = 0\n",
    "\n",
    "    f_score = {n: float('inf') for n in G.nodes}\n",
    "    f_score[start] = heuristic[start]\n",
    "\n",
    "    # 우선순위 큐: (f_score, g_score, node)\n",
    "    open_set = []\n",
    "    heapq.heappush(open_set, (f_score[start], 0, start))\n",
    "    visited = set()\n",
    "    came_from = {}\n",
    "\n",
    "    num_iter = 0\n",
    "\n",
    "    while open_set:\n",
    "        num_iter += 1\n",
    "        storage += len(open_set) + len(visited) + len(came_from)\n",
    "        f_cur, g_cur, current = heapq.heappop(open_set)\n",
    "\n",
    "        # 목표 도달\n",
    "        if current == goal:\n",
    "            # 경로 재구성\n",
    "            path = [current]\n",
    "            while current in came_from:\n",
    "                current = came_from[current]\n",
    "                path.append(current)\n",
    "            return path[::-1], g_cur, count, storage, list(checking_edges)\n",
    "        visited.add(current)\n",
    "        # 이웃 탐색\n",
    "        for nbr in G.neighbors(current):\n",
    "            count = count + 1\n",
    "            if nbr in visited or nbr in obstacles:\n",
    "                continue\n",
    "            \n",
    "            tentative_g = g_score[current] + G[current][nbr]['weight']\n",
    "            checking_edges.add((current, nbr))\n",
    "            if tentative_g < g_score[nbr]:\n",
    "                came_from[nbr] = current\n",
    "                g_score[nbr] = tentative_g\n",
    "                f_score[nbr] = tentative_g + heuristic[nbr]\n",
    "                heapq.heappush(open_set, (f_score[nbr], tentative_g, nbr))\n",
    "\n",
    "    # 경로가 없을 때\n",
    "    return None, None, count, storage, list(checking_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc18a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_astar(G, start, goal, heuristic, llm, obstacles=None):\n",
    "    nodes = list(G.nodes)\n",
    "    llm = list(set(llm))\n",
    "    T = []\n",
    "    for ts in llm:\n",
    "        if ts in nodes:\n",
    "            T.append(ts)\n",
    "    if obstacles is None:\n",
    "        obstacles = set()\n",
    "    else:\n",
    "        obstacles = set(obstacles)\n",
    "    print(llm)\n",
    "    # T.reverse()\n",
    "    if not T or T[0] != start:\n",
    "        T.insert(0, start)\n",
    "    if T[-1] != goal:\n",
    "        T.append(goal)\n",
    "    T = [t for t in T if t not in obstacles]\n",
    "    t_idx = 1 if len(T) > 1 else 0\n",
    "    t = T[t_idx]\n",
    "    dist_to_t = nx.single_source_dijkstra_path_length(G, t, weight='weight')\n",
    "    count = 0\n",
    "    storage = 0\n",
    "    checking_edges = set()\n",
    "    came_from = {}\n",
    "    visited = set()\n",
    "\n",
    "    g_score = {n: float('inf') for n in G.nodes}\n",
    "    g_score[start] = 0\n",
    "\n",
    "    f_score = {n: float('inf') for n in G.nodes}\n",
    "    f_score[start] = g_score[start] + heuristic[start] + dist_to_t.get(start, float('inf'))\n",
    "\n",
    "    open_set = []\n",
    "    heapq.heappush(open_set, (f_score[start], g_score[start], start))\n",
    "\n",
    "    while open_set:\n",
    "        storage += len(open_set) + len(visited) + len(came_from)\n",
    "        f_cur, g_cur, current = heapq.heappop(open_set)\n",
    "        if current == goal:\n",
    "            path = [current]\n",
    "            while current in came_from:\n",
    "                current = came_from[current]\n",
    "                path.append(current)\n",
    "            return path[::-1], g_cur, count, storage, list(checking_edges), T\n",
    "        visited.add(current)\n",
    "        if current == t and t_idx < len(T) - 1:\n",
    "            t_idx += 1\n",
    "            t = T[t_idx]\n",
    "            dist_to_t = nx.single_source_dijkstra_path_length(G, t, weight='weight')\n",
    "\n",
    "            new_open = []\n",
    "            for _, g_old, node in open_set:\n",
    "                new_f = g_score[node] + heuristic[node] + dist_to_t.get(node, float('inf'))\n",
    "                new_open.append((new_f, g_old, node))\n",
    "            open_set = new_open\n",
    "            heapq.heapify(open_set)\n",
    "\n",
    "\n",
    "        for nbr in G.successors(current):\n",
    "            if nbr in visited or nbr in obstacles:\n",
    "                continue\n",
    "            count += 1\n",
    "            w = G[current][nbr].get('weight', 1)\n",
    "            tentative_g = g_score[current] + w\n",
    "            checking_edges.add((current, nbr))\n",
    "\n",
    "            if tentative_g < g_score[nbr]:\n",
    "                came_from[nbr] = current\n",
    "                g_score[nbr] = tentative_g\n",
    "                f = tentative_g + heuristic[nbr] + dist_to_t.get(nbr, float('inf'))\n",
    "                f_score[nbr] = f\n",
    "                heapq.heappush(open_set, (f, tentative_g, nbr))\n",
    "\n",
    "\n",
    "    return None, None, count, storage, list(checking_edges), T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfb0fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/LLM/lib/python3.13/site-packages/networkx/readwrite/json_graph/node_link.py:287: FutureWarning: \n",
      "The default value will be changed to `edges=\"edges\" in NetworkX 3.6.\n",
      "\n",
      "To make this warning go away, explicitly set the edges kwarg, e.g.:\n",
      "\n",
      "  nx.node_link_graph(data, edges=\"links\") to preserve current behavior, or\n",
      "  nx.node_link_graph(data, edges=\"edges\") for forward compatibility.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"graphs/sejong_bus.json\", \"r\") as json_file:\n",
    "    j = json.load(json_file)\n",
    "\n",
    "G = json_graph.node_link_graph(j)\n",
    "nodes = G.nodes\n",
    "edges = G.edges\n",
    "stations = list(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab31a979",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m experiments_samples_A_star = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     start, goal = np.random.choice(\u001b[43mnodes\u001b[49m, \u001b[32m2\u001b[39m)\n\u001b[32m      6\u001b[39m     start, goal = start.item(), goal.item()\n\u001b[32m      8\u001b[39m     heuristic_table = {station: (((G.nodes[goal][\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m] - G.nodes[station][\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m]) ** \u001b[32m2\u001b[39m + (G.nodes[goal][\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m] - G.nodes[station][\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m]) ** \u001b[32m2\u001b[39m) ** \u001b[32m0.5\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m station \u001b[38;5;129;01min\u001b[39;00m nodes}\n",
      "\u001b[31mNameError\u001b[39m: name 'nodes' is not defined"
     ]
    }
   ],
   "source": [
    "# table(random) A*\n",
    "experiments_samples_A_star = []\n",
    "\n",
    "while True:\n",
    "    start, goal = np.random.choice(nodes, 2)\n",
    "    start, goal = start.item(), goal.item()\n",
    "\n",
    "    heuristic_table = {station: (((G.nodes[goal]['x'] - G.nodes[station]['x']) ** 2 + (G.nodes[goal]['y'] - G.nodes[station]['y']) ** 2) ** 0.5) for station in nodes}\n",
    "\n",
    "    path, cost, count, storage, checking_edges = astar(G, start, goal, heuristic_table)\n",
    "    if path is not None:\n",
    "        experiments_samples_A_star.append({'point' : (start, goal), 'path':path, 'cost':cost, 'storage':storage, 'count':count, 'checking_edges' : len(checking_edges)})\n",
    "    if len(experiments_samples_A_star)>150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ebe73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d841256be3a419f8deab83fbccb5088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B\", device_map='auto', load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae59ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6800376, 6800377, 6800378, 6800379, 6800380, 6800381, 6800382, 6800383, 6800384, 6800385, 6800386, \n"
     ]
    }
   ],
   "source": [
    "start, goal = 6800326, 6800463\n",
    "\n",
    "n_points = 7\n",
    "prompt = graph_to_prompt(j, start, goal, n_points)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True).to(model.device)\n",
    "        \n",
    "input_ids = inputs[\"input_ids\"]\n",
    "inputs[\"input_ids\"] = inputs[\"input_ids\"].to(model.device)\n",
    "inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,\n",
    "    )\n",
    "generated_token = outputs[:, input_ids.shape[-1] :]\n",
    "result = tokenizer.batch_decode(generated_token, skip_special_tokens=True)[0]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.config.max_position_embeddings\n",
    "llm_model = Request_llm('deepseek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 7\n",
    "prompt = graph_to_prompt(j, start, goal, n_points)\n",
    "llm_model.tokenizer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table(random) LLM-A*\n",
    "num_request = 7\n",
    "llm_model.model.eval()\n",
    "ctx = torch.inference_mode  # 또는 torch.no_grad\n",
    "experiments_samples_LLM_A_star = []\n",
    "\n",
    "with ctx():\n",
    "    for i, sample in (enumerate(tqdm(experiments_samples_A_star[:10]))):\n",
    "        start, goal = sample['point']\n",
    "\n",
    "        heuristic_table = {station: (((G.nodes[goal]['x'] - G.nodes[station]['x']) ** 2 + (G.nodes[goal]['y'] - G.nodes[station]['y']) ** 2) ** 0.5) for station in nodes}\n",
    "\n",
    "        waypoints = llm_model.get_waypoints(j, start, goal, num_request)\n",
    "        \n",
    "        path, cost, count, storage, checking_edges, waypoints_using_llm = llm_astar(G, start, goal, heuristic_table, waypoints)\n",
    "        if path is not None:\n",
    "            experiments_samples_LLM_A_star.append(\n",
    "                {'point' : (start, goal), \n",
    "                'path':path, \n",
    "                'cost':cost, \n",
    "                'count':count, \n",
    "                'storage':storage,\n",
    "                'checking_edges' : len(checking_edges),\n",
    "                'waypoints_of_llm' : waypoints,\n",
    "                'waypoints_using_llm' : waypoints_using_llm,\n",
    "                'success' : (len(waypoints) <= num_request + 1)\n",
    "                })\n",
    "            cost_ratio = 1 - cost/sample['cost']\n",
    "            count_ratio = 1 - count/sample['count']\n",
    "            storage_ratio = 1 - storage/sample['storage']\n",
    "            checking_edges_ratio = 1 - len(checking_edges)/sample['checking_edges']\n",
    "            print(\"Sample \", i)\n",
    "            print(\"Count ratio       :\", count_ratio)\n",
    "            print(\"Storage ratio    \",  storage_ratio)\n",
    "            print(\"Cost ratio        :\", cost_ratio)\n",
    "            print(\"# of Checking edges ratio:\", checking_edges_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = experiments_samples_LLM_A_star\n",
    "\n",
    "fields = ['point', 'path', 'cost', 'count', 'checking_edges', 'waypoints_of_llm', 'waypoints_using_llm', 'success']\n",
    "\n",
    "with open('Experiments_result/LLM_A_star_fewshot.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Experiments_result/A_star.csv')\n",
    "data.loc[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b971c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ast.literal_eval(data[0]['point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6590c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_samples_A_star[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Experiments_result/A_star.json', 'w') as jsonfile:\n",
    "\tjson.dump(experiments_samples_A_star, jsonfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074694e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Experiments_result/A_star.json', 'r') as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b19bab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
